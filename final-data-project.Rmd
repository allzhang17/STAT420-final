---
title: "An Analysis of Test Scores Against Various Social and Economic Factors"
author: "Allison Zhang, Kobe Dela Cruz, Nishant Balepur"
date: "11/30/2020"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

# Introduction

The purpose of our project is to effectively predict the test scores of high schoolers in 1986 based on a large number of predictor variables, some of which are gender, ethnicity, family income, etc. 


Our ultimate goal for this project is to reason through the different predictor variables to find the model that can best predict high school test scores.


## Dataset

We will be using the College Distance dataset from the AER package that is found on this following site: https://rdrr.io/cran/AER/man/CollegeDistance.html

The data was obtained by the Department of Education, and contains many different social and economic variables, including: gender, ethnicity, whether or not the mother/father went to college, if the family owns their home, county, unemployment rate, and more.

We first load 'college_distance.csv' into R along with some required packages:

```{r warning = FALSE, echo = TRUE, results = "hide"}
library(readr)
library(knitr)
library(faraway)
library(lmtest)
library(zoo)
library(ggplot2)
#install.packages("reshape2")
library(reshape2)
#install.packages("rsq")
library(rsq)
collegedistance = read.csv("CollegeDistance.csv")
```
This dataset has a total of 4739 observations of 15 variables (14 predictors and 1 response). The said variables are:

- `gender`: a factor indicating gender
- `ethnicity`: factor indicating ethnicity (African-American, Hispanic or other)
- `score`: base year composite test score
- `fcollege`: factor. Is the father a college graduate?
- `mcollege`: factor. Is the mother a college graduate?
- `home`: factor. Does the family own their home?
- `urban`: factor. Is the school in an urban area?
- `unemp`: country unemployment rate in 1980
- `wage`: state hourly wage in manufacturing in 1980
- `distance`: distance from 4-year college (in 10 miles)
- `tuition`: average state 4-year college tuition (in 1000 USD)
- `education`: number of years of education
- `income`: factor. Is the family income above 25,000 USD per year?
- `region`: factor indicating region (West or other)


The dataset meets all of the set criteria for the project. Now lets look for missing values for our next step in data cleaning.

## Data Cleaning

### Loading in the Dataset

We received a warning message when loading the data that there was an unnamed column. Taking a look, we saw that R created another x-coordinate column. We will get rid of that.
```{r}
collegedistance = collegedistance[ , -1]
head(collegedistance)
```
Next, we will see if there are any missing values within our dataset.
```{r}
sum(is.na(collegedistance))
```
Great! We see that there are no missing values in our dataset, so no additional work needs to be done there. 

### Converting Factor Variables

We also notice that many of our variables are factor variables, so we need to convert those to binary values. The dependent variables will be converted as follows:

- `ethnicity`: converted into two variables, `hispanic` and `afam`. A value of 1 means the student is Hispanic or African-American, respectively
- `gender`: 1 for male, 0 for female
- `fcollege`: 1 for yes, 0 for no
- `mcollege`: 1 for yes, 0 for no
- `home`: 1 for yes, 0 for no
- `urban`: 1 for yes, 0 for no
- `income`: 1 for high, 0 for low
- `region`: 1 for west, 0 for other

```{r}

# convert the ethnicity label and remove the old one
collegedistance$hispanic = 1 * (collegedistance$ethnicity == "hispanic")
collegedistance$afam = 1 * (collegedistance$ethnicity == "afam")
collegedistance = collegedistance[-1 * which(names(collegedistance) == "ethnicity")]

# convert the rest of the labels with automation
convert_labels = c(c("fcollege", "yes"), c("mcollege", "yes"), c("home", "yes"),
                   c("urban", "yes"), c("income", "high"), c("region", "west"),
                   c("gender", "male"))

# loop through each label
for (label_index in seq(1, length(convert_labels), 2)) {
  
  # get the column name and positive label name
  col_name = convert_labels[label_index]
  positive_label = convert_labels[label_index + 1]

  # convert the label appropriately
  collegedistance[col_name] = 1 * (collegedistance[col_name] == positive_label)
}

```

Now let's take a look at the data with our adjusted variables

```{r}
head(collegedistance)
```

It appears that none of the other variables need to be changed 

### Identifying Correlation

Before we start creating our models, we'll take a look at our variables to ensure that there is no correlation affecting our model

To do this, we'll need a correlation matrix

```{r warning = FALSE}
# get the correlation matrix
college_cor = round(cor(collegedistance), 2)

# remove the NA values
college_cor[which(is.na(college_cor))] = 0

# summary of the correlation
head(college_cor)
```


This table is a little difficult to analyze, so we'll convert it to a visual heatmap

```{r include = FALSE}

# code reference: http://www.sthda.com/english/wiki/ggplot2-quick-correlation-matrix-heatmap-r-software-and-data-visualization

# function to reorder the corelation matrix
reorder_cormat = function(cormat){
  # Use correlation between variables as distance
  dd <- as.dist((1-cormat)/2)
  hc <- hclust(dd)
  cormat <- cormat[hc$order, hc$order]
}

# Get upper triangle of the correlation matrix
get_lower_tri = function(cormat){
    cormat[upper.tri(cormat)] <- NA
    return(cormat)
}

# Get upper triangle of the correlation matrix
get_upper_tri <- function(cormat){
    cormat[lower.tri(cormat)]<- NA
    return(cormat)
}

# use our helper functions to format the correlation matrix
melted_cormat = melt(get_upper_tri(reorder_cormat(college_cor)), na.rm = TRUE)

# create the heat map
ggheatmap = ggplot(melted_cormat, aes(Var2, Var1, fill = value)) +
            ggtitle("Correlation Matrix Heatmap") +
            xlab("Variable 1") + ylab("Variable 2") +
            geom_tile(color = "white") +
            scale_fill_gradient2(low = "blue", high = "red", mid = "white", 
                                 midpoint = 0, limit = c(-1,1), space = "Lab", 
                                 name="Pearson\nCorrelation") +
            theme_minimal() + 
            theme(axis.text.x = element_text(angle = 45, vjust = 1, 
                                             size = 18, hjust = 1),
                  axis.text.y = element_text(size = 18),
                  axis.title = element_text(size = 25),
                  plot.title = element_text(size = 30, hjust = 0.5),
                  legend.text = element_text(size = 16),
                  legend.title = element_text(size = 20),
                  legend.key.width = unit(1, "cm"),
                  legend.key.height = unit(2.5, "cm")) +
            coord_fixed()
            
```

```{r fig.height = 10, fig.width = 15}
# print our heatmap (code hidden above)
ggheatmap
```

From the above heatmap, we see that there are only a few variables that could be problematic. To further validate our findings, we'll explore the variance inflation factors for a simple additive model

```{r warning = FALSE}
simple_add = lm(score ~ ., data = collegedistance)
vif(simple_add)
```

We can see from this model that there are **`r sum(vif(simple_add) >= 5)`** values greater than 5, so there appears to be no collinearity between our dependent variables

### Variable Intuition

We will now do a brief summary of all of our variables, just to see if there are any more than we can remove

```{r}
colMeans(collegedistance)
```

Since all of these `arithmetic means` look good (especially since none of the factor variables have a mean of 0 or 1), we can start to build our model

## Method Exploration

### Dependent Variable Transformation

First, we'll take a look at our dependent variable, `score` to see if any transformations are necessary. We expect to see a normal distribution

The function `buildHistogram()` will help us see this visually

```{r}
buildHistogram = function(values, title) {
  freq_hist = hist((values - mean(values)) / sd(values),
                   col = "darkorange",
                   xlab = "Score",
                   main = title)
  
  multiplier = freq_hist$counts / freq_hist$density
  density = density(values)
  density$y = density$y * multiplier[1]
  
  x = seq(-3, 3, length.out = length(values))
  
  curve(multiplier * dnorm(x), col = "dodgerblue", lwd=2, add=TRUE, yaxt="n")
}
```


```{r warning = FALSE}
buildHistogram(collegedistance$score, "Frequency of Normalized Score")
```

The above plot might suffice, but we will look at an additional transformation to hopefully see a more normalized output

```{r warning = FALSE}
buildHistogram(collegedistance$score^0.75, "Frequency of Normalized Score - Transformed")
```

The transformation of score to the pwoer of `0.75` we decided might be a valid transformation, so when we're building our model we'll keep these findings in mind

### Independent Variable Transformation

We'll now see if any independent variable transformations, specifically for our numeric variables, might be necessary by using scatter plots and our own statistical judgement and intuition as a starting point

To start, we'll create a helper function to help us visualize our variables plotted against score

```{r}
buildScatterPlot = function(dep_label, ind_label, color1, color2 = color, interaction = TRUE) {
  plot(y = unlist(collegedistance[tolower(dep_label)]),
       x = unlist(collegedistance[tolower(ind_label)]),
       xlab = ind_label,
       ylab = dep_label,
       main = paste(dep_label, " vs ", ind_label),
       col = ifelse(interaction, color1, color2),
       pch = 20,
       cex = 1,
       cex.lab = 2,
       cex.axis = 2,
       cex.main = 2)
}
```

Now we'll make the plots for our numeric variables

```{r fig.height = 10, fig.width = 15}
par(mfrow = c(2, 2))
buildScatterPlot("Score", "Wage", "dodgerblue")
buildScatterPlot("Score", "Unemp", "darkorange")
buildScatterPlot("Score", "Distance", "firebrick")
buildScatterPlot("Score", "Tuition", "mediumpurple")
```

There appears to be no transformations needed, and unforunately our data looks fairly random, possibly indicating that there is no strong trend between our variables

Before we give up, we will repeat the process for our interaction variables. The function `createInteractionPlots()` will save us from needing to repeat our code

```{r}
createInteractionPlots = function(interaction) {
    par(mfrow = c(2, 2))
    buildScatterPlot("Score", "Wage", "dodgerblue" , "cadetblue1",
                     unlist(collegedistance[interaction]))
    buildScatterPlot("Score", "Unemp", "seagreen1", "palegreen4",
                     unlist(collegedistance[interaction]))
    buildScatterPlot("Score", "Distance", "firebrick", "tomato",
                     unlist(collegedistance[interaction]))
    buildScatterPlot("Score", "Tuition", "mediumpurple", "orchid2",
                     unlist(collegedistance[interaction]))
}
```

<br />

#### Gender

```{r fig.height = 10, fig.width = 15}
createInteractionPlots("gender")
```

<br />

#### Father Went to College?

```{r fig.height = 10, fig.width = 15}
createInteractionPlots("fcollege")
```

<br />

#### Mother Went to College?

```{r fig.height = 10, fig.width = 15}
createInteractionPlots("mcollege")
```

<br />

#### Home

```{r fig.height = 10, fig.width = 15}
createInteractionPlots("home")
```

<br />

#### Urban

```{r fig.height = 10, fig.width = 15}
createInteractionPlots("urban")
```

<br />

#### Income

```{r fig.height = 10, fig.width = 15}
createInteractionPlots("income")
```

<br />

#### Region

```{r fig.height = 10, fig.width = 15}
createInteractionPlots("region")
```

<br />

#### Hispanic

```{r fig.height = 10, fig.width = 15}
createInteractionPlots("hispanic")
```

<br />

#### African American

```{r fig.height = 10, fig.width = 15}
createInteractionPlots("afam")
```

Looking at all of the above charts, we notice that there is no strong interaction trend when only looking at the combination of two variables. Hence, it's difficult for us to visually extract trends, and we must now rely on statistcal metrics to continue


<hr />

To make it easier for us to norrow down our options when selecting a model, we will check our assumptions using a variety of functions/tests.

```{r}
# performs the Breusch-Pagan test
get_bp = function(model, alpha = 0.01) {
  bptest_res = bptest(model)
  decision = bptest_res$p.value < alpha
  return(list(decision = ifelse(decision, "Reject", "Fail To Reject"),
              stat = bptest_res$statistic, pvalue = bptest_res$p.value))
}

# performs the Shapiro-Wilk test
get_shapiro = function(model, alpha = 0.01) {
  shapiro_res = shapiro.test(resid(model))
  decision = shapiro_res$p.value < alpha
  return(list(decision = ifelse(decision, "Reject", "Fail to Reject"),
              stat = shapiro_res$statistic, pvalue = shapiro_res$p.value))
}

# finds the RMSE of our model
get_rmse = function(model) {
  sqrt(mean((resid(model) / (1 - hatvalues(model))) ^ 2))
}

# combines the above helper methods into a readable format
evaluate = function(name, model, response_func = I, data = collegedistance) {
  set.seed(21)
  data.frame(Model = name,
             rmse = get_rmse(model),
             adj_r2 = rsq(model, adj = TRUE),
             aic = AIC(model),
             bic = BIC(model),
             coeff = length(coef(model)),
             shapiro_dec = get_shapiro(model)$decision,
             bp_dec = get_bp(model)$decision)
}
```

Next, let's start creating some basic models.
```{r}
# Start with a model with all predictors.
full_score_model = lm(score ~ ., data = collegedistance)
full_score_eval = evaluate("All Additive Model", full_score_model)
full_score_eval
```
Now that we have a baseline quality criterion, let's compare this to a model with the predictors chosen from the heat map. 
```{r}
# Comparing with a model using selected predictors.
smaller_add_model = lm(score ~ education + fcollege + mcollege + wage + tuition + gender + home + income, data = collegedistance)
smaller_add_eval = evaluate("Smaller Add Model", smaller_add_model)
smaller_add_eval
```
We see that the additive model with chosen predictors has a lower adjusted $r^2$ and a higher BIC. Let's try comparing it to the interaction model.
```{r}
full_int_mod = lm(score ~ .^2, data = collegedistance)
full_int_eval = evaluate("All Interaction Model", full_int_mod)
full_int_eval
```
We see that the interaction model has a higher adjusted $r^2$, let's use an ANOVA $F$-test to choose between the All Additive Model and All Interaction Model.
```{r}
anova(full_score_model, full_int_mod)
```
The p-value is extremely low (0.0003279), so between the two, we choose the interaction model to begin narrowing down our predictors.

Also, we can see that the interaction model had a smaller AIC than BIC, so we will move forward with using backwards AIC to search for and narrow down parameters.

## Model Building + Testing

We first begin by using backwards AIC to do a backwards parameter search.
```{r eval = FALSE}
full_int_mod = lm(score ~ . ^ 2, data = collegedistance)
distance_aic = step(full_int_mod, direction = "backward", trace = 0)
coef(distance_aic)
```
```{r eval = FALSE}
distance_larger = lm(score ~ .^2 + I(gender^2) + I(fcollege^2) + I(mcollege^2) + I(home^2) + I(urban^2) + I(unemp^2) + I(wage^2) + I(distance^2) + I(tuition^2) + I(education^2) + I(hispanic^2) + I(afam^2), data = collegedistance)
distance_aic2 = step(distance_larger, direction = "backward", trace = 0)
coef(distance_aic2)
```
```{r eval = FALSE}
length(coef(distance_aic))
```
```{r eval = FALSE}
length(coef(distance_aic2))
```
```{r eval = FALSE}
evaluate("All Interactions", distance_aic)
```
```{r eval = FALSE}
evaluate("Some interactions", distance_aic2)
```
```{r eval = FALSE}
summary(distance_aic2)
```

By looking at the parameters returned by backwards AIC, we can narrow them down to create a 'better' model.
```{r eval = FALSE}
# ungodly model
test_model3 = lm(score ~ gender + fcollege + fcollege:mcollege + home + urban + wage + distance + I(distance^(0.25)) + tuition + education + income + hispanic + afam + I(distance^2) + I(tuition^2) + I(education^2) + gender:home + gender:wage + gender:education + gender:hispanic + gender:afam + fcollege:income + mcollege:income + home:urban + home:unemp + home:distance + unemp:distance + unemp:hispanic + wage:hispanic + wage:afam + distance:hispanic + distance:afam + tuition:education + income:hispanic + I(tuition^3) + log(tuition) + I(education^3)+ log(education) + I(tuition^0.25) + I(education^0.25) + I(wage^2) + gender:I(wage^2) + gender:I(log(wage)) + hispanic:I(wage^2) + hispanic:I(log(wage)) + home:I(unemp^2) + home:I(unemp^0.25) + home:I(log(unemp)) + home:I(distance^2) + home:I(distance^0.25) +  unemp:I(distance^2) + unemp:I(distance^0.25) + hispanic:I(distance^2) + hispanic:I(distance^0.25) + afam:I(distance^2) + afam:I(distance^0.25) + region, data = collegedistance)

evaluate("Test Model 3", test_model3)
```
By running this model through our quality criterion function, we see that our LOOCV-RMSE, adjusted $R^2$, AIC and BIC numbers have improved significantly. However, the assumption tests still fail.
```{r eval=FALSE}
summary(test_model3)
```

Finally, we will perform ANOVA tests on all three of our contenders to erase any doubt when choosing the best model to move forward with.
```{r}
anova(distance_aic, distance_aic2, test = "F")
```

- The null hypothesis: There is a small difference in RSS between the two models.
- The test statistic: 5.8834
- The distribution of the test statistic under the null hypothesis: F-Distribution with 5 and 4704 DF
- The p-value: 1.993e-05
- A decision: We reject the null hypothesis at $\alpha = 0.10$ 
- Preferred model: 'distance_aic2'

Now, let's compare 'distance_aic2' with 'test_model3'.
```{r}
anova(distance_aic2, test_model3, test = "F")
```
- The null hypothesis: There is a small difference in RSS between the two models.
- The test statistic: 2.1876
- The distribution of the test statistic under the null hypothesis: F-Distribution with 23 and 4658 DF
- The p-value: 0.0008783
- A decision: We reject the null hypothesis at $\alpha = 0.10$ 
- Preferred model: 'test_model3'

Great! We've confirmed that 'test_model3' is indeed the preferred model of all three, and now we will move on to finalizing the model.


### Finalizing the Model

### Analyzing Unusual Points

Before we finalize our model, we also need to make sure that we remove all of the unusual points in our data set

The function `getUnsualPoints()` below will help us streamline the process of finding these points

```{r}
# finds the unusual points (leverage, residual, influential) of a certain model
getUnusualPoints = function(model) {
  # calculate leverage
  num_high_leverage = sum(hatvalues(model) > 2 * mean(hatvalues(model)))
  
  # calculate large residuals
  num_large_residual = sum(abs(rstandard(model)) > 2)
  
  # calculate influential
  cooks_dist = cooks.distance(model)
  num_influential = sum(cooks_dist > 4 / length(cooks_dist))
  
  # return the values
  return(list(leverage = num_high_leverage,
              residual = num_large_residual,
              influential = num_influential))
}
```

Let's run the above function with our current best model

```{r}
#TODO: Change to current best model
unusual_point_data = getUnusualPoints(test_model3)
print(unusual_point_data)
```

We find that we have **`r unusual_point_data$influential`** unusual points, so we will remove them and create our final model

We will repeat this process until the unusual points are minimized to a certain value, in our case 5

```{r}

final_model = test_model3
collegedistance_cleaned = collegedistance

while(nrow(collegedistance_cleaned) > 2000) {
  
  # calculate influence indexes
  infl_indexes = which(cooks.distance(final_model) > 4 /
                         length(cooks.distance((final_model))))
  
  # calculate residual indexes
  lev_indexes = which(abs(rstandard(final_model)) > 2)
  
  # get all unique subset indexes
  subset_indexes = unique(c(infl_indexes, lev_indexes, resid_indexes))
  
  if (nrow(collegedistance_cleaned) - length(subset_indexes) < 2000) {
    break
  }
  
  collegedistance_cleaned = collegedistance_cleaned[-1 * subset_indexes,]
  
  
  
  final_model = lm(score ~ gender + fcollege + mcollege + home + urban +
                           unemp + wage + distance + tuition + education +
                           income + region + hispanic + afam + I(distance^2) +
                           I(education^2) + gender:home + gender:urban + 
                           gender:wage + gender:tuition + gender:education + 
                           gender:region + gender:hispanic + gender:afam + 
                           fcollege:income + mcollege:income + home:urban + 
                           home:unemp + home:distance + home:tuition + 
                           home:region + urban:unemp + unemp:hispanic + 
                           wage:region + wage:afam + distance:hispanic + 
                           distance:afam + tuition:region + education:region + 
                           income:hispanic, data = collegedistance_cleaned)
}
```

After performing this final step, we obtain the following test results:

```{r}
final_eval_data = evaluate("Final Model", final_model)
print(final_eval_data)
```

We are left with a model with `RMSE` of **`r final_eval_data$rmse`** and `Adjusted R Squared` of **`r final_eval_data$adj_r2`**

## Model Results

### Model Diagnostics

We will now test the various diagnostics and assumptions of our final model

The helper functions below will help us create the visuals we need:

```{r}

# create the fitted versus residuals scatter plot
createFittedResidualsPlot = function(model) {
  plot(fitted(model), resid(model), col = "grey", pch = 20,
       xlab = "Fitted", ylab = "Residuals", 
       main = "Fitted vs Residuals of Final Model")
  abline(h = 0, col = "darkorange", lwd = 2)
}

# creates a histogram of residuals
createResidualHistogram = function(model) {
  hist(resid(model),
     xlab   = "Residuals",
     main   = "Histogram of Residuals for Final Model",
     col    = "darkorange",
     border = "dodgerblue",
     breaks = 20)
}

# creates a Q-Q plot
createQQPlot = function(model) {
  qqnorm(resid(model), main = "Normal Q-Q Plot", col = "darkgrey", 
         cex.main = 2, cex.axis = 2, cex.lab = 2)
  qqline(resid(model), col = "dodgerblue", lwd = 2)
}
```

First, we will take a look at the residuals of our model

```{r fig.height = 5, fig.width = 15}
par(mfrow = c(1, 2))
createResidualHistogram(final_model)
createFittedResidualsPlot(final_model)
```

Our residuals have a fairly normal distribution, and don't seem to have any general trend in the scatter plot

We can double-check this with the Breusch-Pagan test

```{r}
# obtain our B-P test results
final_bp = get_bp(final_model)
print(final_bp)
```

Unforunately, when we run this test, we obtain a test statistic of **`r final_bp$stat`**, giving us a p-value of **`r final_bp$pvalue`**. It makes sense that we obtained visual results for homoscedasticity but statistical results for heterocedasticity, as we have shown that our data was difficult to predict with a model

<br />

Now we'll take a look at the Q-Q plot for our final model

```{r fig.height = 10, fig.width = 15}
createQQPlot(final_model)
```

We notice that visually, our Q-Q plot is slightly off and not perfectly fitting our data. We can verify that this assumption is indeed violated with the Shapiro-Wilks test

```{r}
final_shapiro = get_shapiro(final_model)
print(final_shapiro)
```

This test gives us a test statistic of **`r final_shapiro$stat`** and thus a p-value of **`r final_shapiro$pvalue`**, meaning that we **reject** $H_0$. Hence, this assumption is violated

<br />

Finally, we'll look at the number of unusual points in our data

```{r}
final_unusual = getUnusualPoints(final_model)
print(final_unusual)
```

Our data has **`r final_unusual$leverage`** points of high leverage, **`r final_unusual$residual`** points of with large residual, and **`r final_unusual$infleuntial`** influential points. Even with intensive data cleaning, these results prove our suspicions that the data selected is highly volatile

## Discussion

## Appendix