---
title: "An Analysis of Test Scores Against Various Social and Economic Factors"
author: "Allison Zhang, Kobe Dela Cruz, Nishant Balepur"
date: "11/30/2020"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

# Introduction

The purpose of our project is to effectively predict the test scores of high schoolers in 1986 based on a large number of predictor variables, some of which are gender, ethnicity, family income, etc. 


Our ultimate goal for this project is to reason through the different predictor variables to find the model that can best predict high school test scores.


## Dataset

We will be using the College Distance dataset from the AER package that is found on this following site: https://rdrr.io/cran/AER/man/CollegeDistance.html

The data was obtained by the Department of Education, and contains many different social and economic variables, including: gender, ethnicity, whether or not the mother/father went to college, if the family owns their home, county, unemployment rate, and more.

We first load 'college_distance.csv' into R along with some required packages:

```{r warning = FALSE, echo = TRUE, results = "hide"}
library(readr)
library(knitr)
library(faraway)
library(lmtest)
library(zoo)
library(ggplot2)
#install.packages("reshape2")
library(reshape2)
#install.packages("rsq")
library(rsq)
collegedistance = read.csv("CollegeDistance.csv")
```
This dataset has a total of 4739 observations of 15 variables (14 predictors and 1 response). The said variables are:

- `gender`: a factor indicating gender
- `ethnicity`: factor indicating ethnicity (African-American, Hispanic or other)
- `score`: base year composite test score
- `fcollege`: factor. Is the father a college graduate?
- `mcollege`: factor. Is the mother a college graduate?
- `home`: factor. Does the family own their home?
- `urban`: factor. Is the school in an urban area?
- `unemp`: country unemployment rate in 1980
- `wage`: state hourly wage in manufacturing in 1980
- `distance`: distance from 4-year college (in 10 miles)
- `tuition`: average state 4-year college tuition (in 1000 USD)
- `education`: number of years of education
- `income`: factor. Is the family income above 25,000 USD per year?
- `region`: factor indicating region (West or other)


The dataset meets all of the set criteria for the project. Now lets look for missing values for our next step in data cleaning.

## Data Cleaning

We received a warning message when loading the data that there was an unnamed column. Taking a look, we saw that R created another x-coordinate column. We will get rid of that.
```{r}
collegedistance = collegedistance[ , -1]
head(collegedistance)
```
Next, we will see if there are any missing values within our dataset.
```{r}
sum(is.na(collegedistance))
```
Great! We see that there are no missing values in our dataset, so no additional work needs to be done there. 

<hr />

We also notice that many of our variables are factor variables, so we need to convert those to binary values. The dependent variables will be converted as follows:

- `ethnicity`: converted into two variables, `hispanic` and `afam`. A value of 1 means the student is Hispanic or African-American, respectively
- `gender`: 1 for male, 0 for female
- `fcollege`: 1 for yes, 0 for no
- `mcollege`: 1 for yes, 0 for no
- `home`: 1 for yes, 0 for no
- `urban`: 1 for yes, 0 for no
- `income`: 1 for high, 0 for low
- `region`: 1 for west, 0 for other

```{r}

# convert the ethnicity label and remove the old one
collegedistance$hispanic = 1 * (collegedistance$ethnicity == "hispanic")
collegedistance$afam = 1 * (collegedistance$ethnicity == "afam")
collegedistance = collegedistance[-1 * which(names(collegedistance) == "ethnicity")]

# convert the rest of the labels with automation
convert_labels = c(c("fcollege", "yes"), c("mcollege", "yes"), c("home", "yes"),
                   c("urban", "yes"), c("income", "high"), c("region", "west"),
                   c("gender", "male"))

# loop through each label
for (label_index in seq(1, length(convert_labels), 2)) {
  
  # get the column name and positive label name
  col_name = convert_labels[label_index]
  positive_label = convert_labels[label_index + 1]

  # convert the label appropriately
  collegedistance[col_name] = 1 * (collegedistance[col_name] == positive_label)
}

```

Now let's take a look at the data with our adjusted variables

```{r}
head(collegedistance)
```

It appears that none of the other variables need to be changed 

<hr />

Before we start creating our models, we'll take a look at our variables to ensure that there is no correlation affecting our model

To do this, we'll need a correlation matrix

```{r warning = FALSE}
# get the correlation matrix
college_cor = round(cor(collegedistance), 2)

# remove the NA values
college_cor[which(is.na(college_cor))] = 0

# summary of the correlation
head(college_cor)
```


This table is a little difficult to analyze, so we'll convert it to a visual heatmap

```{r include = FALSE}

# code reference: http://www.sthda.com/english/wiki/ggplot2-quick-correlation-matrix-heatmap-r-software-and-data-visualization

# function to reorder the corelation matrix
reorder_cormat = function(cormat){
  # Use correlation between variables as distance
  dd <- as.dist((1-cormat)/2)
  hc <- hclust(dd)
  cormat <- cormat[hc$order, hc$order]
}

# Get upper triangle of the correlation matrix
get_lower_tri = function(cormat){
    cormat[upper.tri(cormat)] <- NA
    return(cormat)
}

# Get upper triangle of the correlation matrix
get_upper_tri <- function(cormat){
    cormat[lower.tri(cormat)]<- NA
    return(cormat)
}

# use our helper functions to format the correlation matrix
melted_cormat = melt(get_upper_tri(reorder_cormat(college_cor)), na.rm = TRUE)

# create the heat map
ggheatmap = ggplot(melted_cormat, aes(Var2, Var1, fill = value)) +
            ggtitle("Correlation Matrix Heatmap") +
            xlab("Variable 1") + ylab("Variable 2") +
            geom_tile(color = "white") +
            scale_fill_gradient2(low = "blue", high = "red", mid = "white", 
                                 midpoint = 0, limit = c(-1,1), space = "Lab", 
                                 name="Pearson\nCorrelation") +
            theme_minimal() + 
            theme(axis.text.x = element_text(angle = 45, vjust = 1, 
                                             size = 18, hjust = 1),
                  axis.text.y = element_text(size = 18),
                  axis.title = element_text(size = 25),
                  plot.title = element_text(size = 30, hjust = 0.5),
                  legend.text = element_text(size = 16),
                  legend.title = element_text(size = 20),
                  legend.key.width = unit(1, "cm"),
                  legend.key.height = unit(2.5, "cm")) +
            coord_fixed()
            
```

```{r fig.height = 10, fig.width = 15}
# print our heatmap (code hidden above)
ggheatmap
```

From the above heatmap, we see that there are only a few variables that could be problematic. To further validate our findings, we'll explore the variance inflation factors for a simple additive model

```{r warning = FALSE}
simple_add = lm(score ~ ., data = collegedistance)
vif(simple_add)
```

We can see from this model that there are **`r sum(vif(simple_add) >= 5)`** values greater than 5, so there appears to be no collinearity between our dependent variables

We will now do a brief summary of all of our variables, just to see if there are any more than we can remove

```{r}
colMeans(collegedistance)
```

Since all of these `arithmetic means` look good (especially since none of the factor variables have a mean of 0 or 1), we can start to build our model

## Method Exploration

### Dependent Variable Transformation

First, we'll take a look at our dependent variable, `score` to see if any transformations are necessary. We expect to see a normal distribution

The function `buildHistogram()` will help us see this visually

```{r}
buildHistogram = function(values, title) {
  freq_hist = hist((values - mean(values)) / sd(values),
                   col = "darkorange",
                   xlab = "Score",
                   main = title)
  
  multiplier = freq_hist$counts / freq_hist$density
  density = density(values)
  density$y = density$y * multiplier[1]
  
  x = seq(-3, 3, length.out = length(values))
  
  curve(multiplier * dnorm(x), col = "dodgerblue", lwd=2, add=TRUE, yaxt="n")
}
```


```{r warning = FALSE}
buildHistogram(collegedistance$score, "Frequency of Normalized Score")
```

The above plot might suffice, but we will look at an additional transformation to hopefully see a more normalized output

```{r warning = FALSE}
buildHistogram(collegedistance$score^0.75, "Frequency of Normalized Score - Transformed")
```

The transformation of score to the pwoer of `0.75` we decided might be a valid transformation, so when we're building our model we'll keep these findings in mind

### Independent Variable Transformation

We'll now see if any independent variable transformations, specifically for our numeric variables, might be necessary by using scatter plots and our own statistical judgement and intuition as a starting point

To start, we'll create a helper function to help us visualize our variables plotted against score

```{r}
buildScatterPlot = function(dep_label, ind_label, color1, color2 = color, interaction = TRUE) {
  plot(y = unlist(collegedistance[tolower(dep_label)]),
       x = unlist(collegedistance[tolower(ind_label)]),
       xlab = ind_label,
       ylab = dep_label,
       main = paste(dep_label, " vs ", ind_label),
       col = ifelse(interaction, color1, color2),
       pch = 20,
       cex = 1,
       cex.lab = 2,
       cex.axis = 2,
       cex.main = 2)
}
```

Now we'll make the plots for our numeric variables

```{r fig.height = 10, fig.width = 15}
par(mfrow = c(2, 2))
buildScatterPlot("Score", "Wage", "dodgerblue")
buildScatterPlot("Score", "Unemp", "darkorange")
buildScatterPlot("Score", "Distance", "firebrick")
buildScatterPlot("Score", "Tuition", "mediumpurple")
```

There appears to be no transformations needed, and unforunately our data looks fairly random, possibly indicating that there is no strong trend between our variables

Before we give up, we will repeat the process for our interaction variables

```{r}
createInteractionPlots = function(interaction) {
    par(mfrow = c(2, 2))
    buildScatterPlot("Score", "Wage", "dodgerblue" , "cadetblue1",
                     unlist(collegedistance[interaction]))
    buildScatterPlot("Score", "Unemp", "seagreen1", "palegreen4",
                     unlist(collegedistance[interaction]))
    buildScatterPlot("Score", "Distance", "firebrick", "tomato",
                     unlist(collegedistance[interaction]))
    buildScatterPlot("Score", "Tuition", "mediumpurple", "orchid2",
                     unlist(collegedistance[interaction]))
}
```


#### Gender

```{r fig.height = 10, fig.width = 15}
createInteractionPlots("gender")
```

#### Father Went to College?

```{r fig.height = 10, fig.width = 15}
createInteractionPlots("fcollege")
```

#### Mother Went to College?

```{r fig.height = 10, fig.width = 15}
createInteractionPlots("mcollege")
```

#### Home

```{r fig.height = 10, fig.width = 15}
createInteractionPlots("home")
```

#### Urban

```{r fig.height = 10, fig.width = 15}
createInteractionPlots("urban")
```

#### Income

```{r fig.height = 10, fig.width = 15}
createInteractionPlots("income")
```

#### Region

```{r fig.height = 10, fig.width = 15}
createInteractionPlots("region")
```

#### Hispanic

```{r fig.height = 10, fig.width = 15}
createInteractionPlots("hispanic")
```

#### African American

```{r fig.height = 10, fig.width = 15}
createInteractionPlots("afam")
```

Looking at all of the above charts, we notice that there is no strong interaction trend when only looking at the combination of two variables. Hence, it's difficult for us to visually extract trends, and must now rely on statistcal metrics on how to continue

<hr />



## Model Results

## Discussion

## Appendix